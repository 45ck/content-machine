# Deep Dive #71: Infrastructure Stack Complete Analysis

**Document ID:** DD-071  
**Date:** 2026-01-02  
**Category:** Infrastructure, Storage, Queues, Captions  
**Status:** Complete  
**Word Count:** ~7,500

---

## Executive Summary

This document provides comprehensive analysis of the **infrastructure stack** for content-machine:

1. **Vector Databases** â€“ Qdrant and Weaviate for semantic search and RAG
2. **Job Queues** â€“ BullMQ for Redis-based task processing
3. **Caption/Subtitle Tools** â€“ auto-subtitle, auto-subtitle-generator for video subtitling
4. **Additional Video Generators** â€“ ai-clips-maker and related tools

---

## 1. Vector Database Solutions

### 1.1 Qdrant

**Source:** `vendor/storage/qdrant/`  
**Language:** Rust  
**License:** Apache 2.0  
**Stars:** 20k+

#### Overview

Qdrant is a **vector similarity search engine** purpose-built for neural network-based matching, semantic search, and RAG applications.

#### Key Features

| Feature                 | Description                                      |
| ----------------------- | ------------------------------------------------ |
| **Filtering**           | JSON payload filtering with any query conditions |
| **Hybrid Search**       | Sparse + dense vectors (BM25 + semantic)         |
| **Quantization**        | Up to 97% RAM reduction                          |
| **Distributed**         | Horizontal scaling with sharding + replication   |
| **SIMD Acceleration**   | x86-64 and ARM Neon optimization                 |
| **Write-Ahead Logging** | Data persistence guarantee                       |

#### Quick Start

```bash
# Docker deployment
docker run -p 6333:6333 qdrant/qdrant
```

```python
from qdrant_client import QdrantClient

# In-memory (testing)
client = QdrantClient(":memory:")

# Persistent local
client = QdrantClient(path="path/to/db")

# Remote server
client = QdrantClient("http://localhost:6333")
```

#### Use Cases for content-machine

| Use Case              | Implementation                                  |
| --------------------- | ----------------------------------------------- |
| **Content Memory**    | Store generated video scripts for deduplication |
| **Trend Memory**      | Remember researched trends to avoid repetition  |
| **Product Knowledge** | RAG for product documentation                   |
| **Similar Content**   | Find similar previous videos                    |

#### MCP Integration

Already have Qdrant MCP server in vendors:

```json
{
  "mcpServers": {
    "qdrant-memory": {
      "command": "uvx",
      "args": ["mcp-server-qdrant"],
      "env": {
        "QDRANT_URL": "http://localhost:6333",
        "COLLECTION_NAME": "content-machine"
      }
    }
  }
}
```

#### Client Libraries

- **Python:** `qdrant-client`
- **JavaScript/TypeScript:** `@qdrant/js-client-rest`
- **Go:** `github.com/qdrant/go-client`
- **Rust:** `qdrant-client`
- **.NET:** `Qdrant.Client`
- **Java:** `io.qdrant:client`

### 1.2 Weaviate

**Source:** `vendor/storage/weaviate/`  
**Language:** Go  
**License:** BSD 3-Clause  
**Stars:** 11k+

#### Overview

Weaviate is a **cloud-native vector database** that stores both objects and vectors, enabling semantic search, RAG, and reranking in a single query interface.

#### Key Features

| Feature                    | Description                                  |
| -------------------------- | -------------------------------------------- |
| **Integrated Vectorizers** | OpenAI, Cohere, HuggingFace, Google built-in |
| **Hybrid Search**          | BM25 + vector in single API call             |
| **Built-in RAG**           | Generative search without additional tooling |
| **Reranking**              | Built-in result reranking                    |
| **Multi-tenancy**          | Native tenant isolation                      |
| **RBAC**                   | Role-based access control                    |

#### Quick Start

```yaml
# docker-compose.yml
services:
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.32.2
    ports:
      - '8080:8080'
      - '50051:50051'
    environment:
      ENABLE_MODULES: text2vec-model2vec
      MODEL2VEC_INFERENCE_API: http://text2vec-model2vec:8080

  text2vec-model2vec:
    image: cr.weaviate.io/semitechnologies/model2vec-inference:minishlab-potion-base-32M
```

```python
import weaviate
from weaviate.classes.config import Configure, DataType, Property

client = weaviate.connect_to_local()

# Create collection with automatic vectorization
client.collections.create(
    name="Article",
    properties=[Property(name="content", data_type=DataType.TEXT)],
    vector_config=Configure.Vectors.text2vec_model2vec(),
)

# Insert with auto-vectorization
articles = client.collections.get("Article")
articles.data.insert_many([
    {"content": "Vector databases enable semantic search"},
    {"content": "Machine learning models generate embeddings"},
])

# Semantic search
results = articles.query.near_text(query="Search by meaning", limit=1)
```

#### Framework Integrations

| Framework  | Integration                          |
| ---------- | ------------------------------------ |
| LangChain  | `langchain-weaviate`                 |
| LlamaIndex | `llama-index-vector-stores-weaviate` |
| Haystack   | `weaviate-haystack`                  |
| CrewAI     | Built-in support                     |
| DSPy       | Direct integration                   |
| N8n        | Weaviate nodes                       |

#### Comparison: Qdrant vs Weaviate

| Feature               | Qdrant                  | Weaviate       |
| --------------------- | ----------------------- | -------------- |
| **Language**          | Rust                    | Go             |
| **Vectorization**     | External                | Built-in       |
| **RAG**               | External                | Built-in       |
| **Filtering**         | JSON payloads           | GraphQL + REST |
| **Best For**          | Performance-critical    | Ease of use    |
| **Memory Efficiency** | Excellent               | Good           |
| **MCP Support**       | Yes (mcp-server-qdrant) | Community      |

#### Recommendation

For content-machine:

- **Primary:** Qdrant (performance, MCP support)
- **Alternative:** Weaviate (built-in RAG, easier setup)

---

## 2. Job Queue Solutions

### 2.1 BullMQ

**Source:** `vendor/job-queue/bullmq/`  
**Language:** TypeScript/Node.js  
**Backend:** Redis  
**License:** MIT

#### Overview

BullMQ is the **fastest, most reliable Redis-based distributed queue** for Node.js. It's used by Microsoft, Vendure, Langfuse, and many others.

#### Key Features

| Feature                       | Description                     |
| ----------------------------- | ------------------------------- |
| **Parent/Child Dependencies** | Job workflows with dependencies |
| **Flow Producer**             | Complex job graphs              |
| **Priorities**                | Job prioritization              |
| **Rate Limiting**             | Control throughput              |
| **Delayed Jobs**              | Schedule for future             |
| **Repeatable Jobs**           | Cron-like scheduling            |
| **Sandboxed Workers**         | Isolated execution              |
| **Events**                    | Global job events               |

#### Quick Start

```typescript
import { Queue, Worker, QueueEvents, FlowProducer } from 'bullmq';

// Create queue
const queue = new Queue('video-generation');

// Add job
await queue.add('render', {
  videoId: 'v123',
  config: { format: 'mp4' },
});

// Process jobs
const worker = new Worker('video-generation', async (job) => {
  if (job.name === 'render') {
    await renderVideo(job.data.videoId, job.data.config);
  }
});

// Listen for completion
const events = new QueueEvents('video-generation');
events.on('completed', ({ jobId }) => {
  console.log(`Job ${jobId} completed`);
});
```

#### Flow Producer (Job Workflows)

```typescript
const flow = new FlowProducer();

await flow.add({
  name: 'final-video',
  queueName: 'render',
  data: { outputPath: '/output/video.mp4' },
  children: [
    {
      name: 'generate-audio',
      queueName: 'audio',
      data: { script: 'Hello world' },
    },
    {
      name: 'generate-captions',
      queueName: 'captions',
      data: { audioPath: '/tmp/audio.mp3' },
    },
    {
      name: 'fetch-assets',
      queueName: 'assets',
      data: { query: 'technology background' },
    },
  ],
});
```

#### content-machine Pipeline with BullMQ

```typescript
// Queue definitions
const queues = {
  trend: new Queue('trend-research'),
  script: new Queue('script-generation'),
  audio: new Queue('audio-synthesis'),
  captions: new Queue('caption-generation'),
  assets: new Queue('asset-fetching'),
  render: new Queue('video-rendering'),
  publish: new Queue('video-publishing'),
};

// Flow for complete video generation
const videoFlow = new FlowProducer();

await videoFlow.add({
  name: 'publish-video',
  queueName: 'video-publishing',
  data: { platform: 'tiktok' },
  children: [
    {
      name: 'render-final',
      queueName: 'video-rendering',
      children: [
        {
          name: 'generate-audio',
          queueName: 'audio-synthesis',
          children: [
            {
              name: 'create-script',
              queueName: 'script-generation',
              children: [
                {
                  name: 'research-trend',
                  queueName: 'trend-research',
                  data: { source: 'reddit' },
                },
              ],
            },
          ],
        },
        {
          name: 'generate-captions',
          queueName: 'caption-generation',
          // Depends on audio completion
        },
        {
          name: 'fetch-assets',
          queueName: 'asset-fetching',
        },
      ],
    },
  ],
});
```

#### Dashboard

Official dashboard at [Taskforce.sh](https://taskforce.sh/):

- Queue overview
- Job inspection
- Retry/promote jobs
- Metrics and statistics

#### Redis Alternatives

BullMQ supports **Dragonfly** as a Redis drop-in replacement:

- Multi-core utilization
- Better memory efficiency
- Faster data structures

---

## 3. Caption/Subtitle Tools

### 3.1 auto-subtitle

**Source:** `vendor/auto-subtitle/`  
**Creator:** Miguel Piedrafita  
**License:** MIT

#### Overview

Automatic subtitle overlay using **Whisper + FFmpeg**. Simple CLI tool for burning subtitles into videos.

#### Installation

```bash
pip install git+https://github.com/m1guelpf/auto-subtitle.git
```

#### Usage

```bash
# Basic usage
auto_subtitle /path/to/video.mp4 -o subtitled/

# With larger model
auto_subtitle /path/to/video.mp4 --model medium

# With translation to English
auto_subtitle /path/to/video.mp4 --task translate
```

#### Model Options

| Model     | Size | Quality       | Speed   |
| --------- | ---- | ------------- | ------- |
| `tiny`    | 39M  | Low           | Fastest |
| `tiny.en` | 39M  | Low (English) | Fastest |
| `base`    | 74M  | Medium        | Fast    |
| `small`   | 244M | Good          | Medium  |
| `medium`  | 769M | Better        | Slow    |
| `large`   | 1.5G | Best          | Slowest |

### 3.2 auto-subtitle-generator

**Source:** `vendor/auto-subtitle-generator/`  
**Creator:** zubu007

#### Overview

Generates **Instagram/TikTok-style subtitles** with word highlighting. GUI-based application.

#### Features

| Feature                 | Status     |
| ----------------------- | ---------- |
| Configurable font       | âœ…         |
| Configurable text size  | âœ…         |
| Configurable font color | âœ…         |
| Y position of text      | âœ…         |
| Word-level timestamps   | ðŸš§ Planned |
| Background color        | ðŸš§ Planned |

#### Stack

- **ASR:** Whisper
- **Video:** MoviePy (no OpenCV)
- **GUI:** Custom Tkinter

#### Future Enhancement

Author notes investigating:

- WhisperX
- stable-ts
- whisper-timestamped

For word-level timestamps.

### 3.3 Subtitle Integration Pattern

Based on the subtitle tools:

```python
from whisperx import load_model, load_audio, transcribe
import ffmpeg

# 1. Transcribe with WhisperX (word-level timestamps)
model = load_model("large-v2", device="cuda")
audio = load_audio("audio.mp3")
result = transcribe(model, audio)

# 2. Generate .ass subtitle file
def generate_ass(segments, output_path):
    """Generate Advanced SubStation Alpha subtitle file."""
    header = """[Script Info]
ScriptType: v4.00+
PlayResX: 1080
PlayResY: 1920

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,80,&HFFFFFF,&H000000,&H000000,&H00000000,1,0,0,0,100,100,0,0,1,4,0,2,10,10,50,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""

    events = []
    for segment in segments:
        start = format_time(segment['start'])
        end = format_time(segment['end'])
        text = segment['text'].strip()
        events.append(f"Dialogue: 0,{start},{end},Default,,0,0,0,,{text}")

    with open(output_path, 'w') as f:
        f.write(header)
        f.write('\n'.join(events))

# 3. Burn subtitles with FFmpeg
ffmpeg.input('video.mp4').output(
    'output.mp4',
    vf=f"ass=subtitles.ass"
).run()
```

---

## 4. Additional Video Tools

### 4.1 ai-clips-maker

**Source:** `vendor/ai-clips-maker/`  
**Creator:** Alperen SÃ¼meroÄŸlu  
**License:** MIT

#### Overview

An **AI-native video engine** that turns long-form content into short, viral-ready clips with:

- Word-level transcription (WhisperX)
- Speaker diarization (Pyannote)
- Face/body-aware cropping
- Scene detection

#### Key Features

| Feature          | Implementation        |
| ---------------- | --------------------- |
| Transcription    | WhisperX (word-level) |
| Speaker ID       | Pyannote.audio        |
| Scene Detection  | PySceneDetect         |
| Video Processing | OpenCV + PyAV         |
| Output Formats   | 9:16, 1:1, 16:9       |

#### Pipeline

```
1. Extract audio from video
2. Transcribe with WhisperX
3. Identify speakers (Pyannote)
4. Detect scene changes
5. Crop around active speaker
6. Export clips in desired format
```

#### Usage

```python
from ai_clips_maker import Transcriber, ClipFinder, resize

# Step 1: Transcribe
transcriber = Transcriber()
transcription = transcriber.transcribe(audio_file_path="/path/to/video.mp4")

# Step 2: Find clips
clip_finder = ClipFinder()
clips = clip_finder.find_clips(transcription=transcription)

# Step 3: Resize with speaker tracking
crops = resize(
    video_file_path="/path/to/video.mp4",
    pyannote_auth_token="your_huggingface_token",
    aspect_ratio=(9, 16)
)
```

#### Tech Stack

| Module          | Technology     | Purpose               |
| --------------- | -------------- | --------------------- |
| Transcription   | WhisperX       | Word-level timestamps |
| Diarization     | Pyannote.audio | Speaker segmentation  |
| Video           | OpenCV + PyAV  | Frame processing      |
| Scene Detection | PySceneDetect  | Shot boundaries       |
| ML Inference    | PyTorch        | Model execution       |

### 4.2 Auto-YouTube-Shorts-Maker

**Source:** `vendor/Auto-YouTube-Shorts-Maker/`  
**Creator:** Binary-Bytes

#### Overview

Simple **topic-to-shorts** generator using OpenAI, gTTS, and MoviePy.

#### Pipeline

```
1. Enter video name
2. AI generates content (or manual entry)
3. gTTS creates speech
4. MoviePy edits video with background gameplay
5. Output saved to generated/
```

#### Requirements

- OpenAI API key (optional, $18 free credits)
- Gameplay videos in `gameplay/` folder

#### Best For

- Quick prototyping
- Simple faceless videos
- Gameplay background content

### 4.3 Auto-YT-Shorts

**Source:** `vendor/Auto-YT-Shorts/`  
**Creator:** pro-grammer-SD

#### Overview

YouTube Shorts creation using AI with documentation and gallery examples.

#### Key Features

- Modular design (planned library conversion)
- Gallery of examples
- Wiki documentation

---

## 5. Infrastructure Architecture

### 5.1 Complete Stack Recommendation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    content-machine Infrastructure               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  STORAGE LAYER                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Qdrant (Vector DB)           Redis (Job Queue)          â”‚  â”‚
â”‚  â”‚  - Content embeddings         - BullMQ backend           â”‚  â”‚
â”‚  â”‚  - Semantic search            - Job state                â”‚  â”‚
â”‚  â”‚  - Trend memory               - Event streaming          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  QUEUE LAYER (BullMQ)                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  trend-research â†’ script-generation â†’ audio-synthesis     â”‚  â”‚
â”‚  â”‚       â†“                                                   â”‚  â”‚
â”‚  â”‚  asset-fetching â†’ caption-generation â†’ video-rendering   â”‚  â”‚
â”‚  â”‚                                  â†“                        â”‚  â”‚
â”‚  â”‚                           video-publishing                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  PROCESSING WORKERS                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Trend Worker    Script Worker    Audio Worker           â”‚  â”‚
â”‚  â”‚  - Reddit MCP    - LangGraph      - Kokoro TTS           â”‚  â”‚
â”‚  â”‚  - pytrends      - Groq LLM       - EdgeTTS fallback     â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  Caption Worker   Render Worker    Publish Worker        â”‚  â”‚
â”‚  â”‚  - WhisperX       - Remotion       - YouTube API         â”‚  â”‚
â”‚  â”‚  - .ass output    - chuk-motion    - TikTok upload       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Docker Compose Template

```yaml
version: '3.8'

services:
  # Vector Database
  qdrant:
    image: qdrant/qdrant
    ports:
      - '6333:6333'
    volumes:
      - qdrant_storage:/qdrant/storage

  # Job Queue Backend
  redis:
    image: redis:7-alpine
    ports:
      - '6379:6379'
    volumes:
      - redis_data:/data

  # Optional: Dragonfly (faster Redis alternative)
  # dragonfly:
  #   image: docker.dragonflydb.io/dragonflydb/dragonfly
  #   ports:
  #     - "6379:6379"

  # BullMQ Dashboard
  bull-board:
    image: deadly0/bull-board
    ports:
      - '3000:3000'
    environment:
      REDIS_HOST: redis
      REDIS_PORT: 6379

volumes:
  qdrant_storage:
  redis_data:
```

### 5.3 Worker Implementation Pattern

```typescript
// workers/trend-worker.ts
import { Worker, Job } from 'bullmq';
import { QdrantClient } from '@qdrant/js-client-rest';

const qdrant = new QdrantClient({ url: 'http://localhost:6333' });

const trendWorker = new Worker(
  'trend-research',
  async (job: Job) => {
    const { source } = job.data;

    // 1. Research trends
    const trends = await researchTrends(source);

    // 2. Check for similar past content
    const similar = await qdrant.search('content-history', {
      vector: await embed(trends[0].title),
      limit: 3,
    });

    // 3. Filter out duplicates
    const unique = trends.filter(
      (t) =>
        !similar.some(
          (s) => s.score > 0.95 // Too similar to past content
        )
    );

    // 4. Store for deduplication
    await qdrant.upsert('content-history', {
      points: unique.map((t, i) => ({
        id: Date.now() + i,
        vector: await embed(t.title),
        payload: { title: t.title, date: new Date() },
      })),
    });

    return { trend: unique[0] };
  },
  { connection: { host: 'localhost', port: 6379 } }
);
```

---

## 6. Document Metadata

| Field            | Value                  |
| ---------------- | ---------------------- |
| **Document ID**  | DD-071                 |
| **Created**      | 2026-01-02             |
| **Author**       | Research Agent         |
| **Status**       | Complete               |
| **Dependencies** | DD-065, DD-067, DD-070 |

---

## 7. Key Takeaways

1. **Qdrant** is the recommended vector database (Rust performance, MCP support, filtering)
2. **Weaviate** is an alternative for easier RAG integration
3. **BullMQ** provides robust job queuing with parent/child workflows
4. **auto-subtitle** is the simplest subtitle solution (Whisper + FFmpeg)
5. **ai-clips-maker** is the most sophisticated long-to-short converter
6. **Docker Compose** template ready for immediate deployment

---

## 8. Quick Reference

### Qdrant Deployment

```bash
docker run -p 6333:6333 qdrant/qdrant
```

### BullMQ Queue

```typescript
const queue = new Queue('my-queue');
await queue.add('job-name', { data: 'here' });
```

### Auto-subtitle

```bash
auto_subtitle video.mp4 --model medium -o subtitled/
```

### ai-clips-maker

```python
from ai_clips_maker import Transcriber, ClipFinder, resize
transcription = Transcriber().transcribe("video.mp4")
clips = ClipFinder().find_clips(transcription)
```
