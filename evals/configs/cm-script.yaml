# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json
#
# content-machine Script Generation Evaluation
# Evaluates the quality of LLM-generated video scripts
#
# Usage:
#   npx promptfoo eval -c evals/configs/cm-script.yaml --env-file .env
#   npx promptfoo eval -c evals/configs/cm-script.yaml --no-cache  # Skip cache
#   npx promptfoo view  # View results in browser

description: content-machine script generation quality evaluation

# The prompt template to test (using Nunjucks syntax)
prompts:
  - |
    You are an expert short-form video scriptwriter. You write engaging scripts for TikTok, Reels, and YouTube Shorts.

    Create a short-form video script about: "{{ topic }}"

    FORMAT: {{ archetype }}

    REQUIREMENTS:
    - Start with a compelling hook that creates curiosity (first 2-3 seconds).
    - 3-7 scenes, each with spoken text, visual direction, and mood.
    - End with a concise call-to-action.
    - Use conversational, TikTok-style language.

    TTS WRITING RULES:
    - Write for spoken delivery at 120-180 WPM.
    - Target length: ~110-140 words (~45 seconds).
    - Total spoken word count must be between 90-150 words.
    - Use short sentences (<=15 words) and one idea per sentence.
    - Each scene should be 2 sentences and ~22-30 words (minimum 20 words per scene).
    - Use contractions and second person ("you", "you'll").
    - Use punctuation for timing (commas = micro-pause, periods = full beat).
    - Normalize text for speech: expand numbers, acronyms, URLs, emails, file paths, and units.
    - Avoid openers like "In this video" or "Today we're going to".
    - Hook must be a single sentence and must not be repeated in scene 1.
    - Scene 1 must continue after the hook with NEW information (no restating or paraphrasing the hook).
    - Each scene text must be at least 20 words.
    - Double-check total word count before responding.
    - Hook must be a statement (no question mark).
    - Do not copy the hook sentence into any scene text.

    {% if archetype == "listicle" %}
    Structure as a numbered list with a hook, 4-5 clear items, and a call-to-action.
    Each item should be two sentences: the tip plus a quick payoff or why.
    Prefix each point with an explicit number label (e.g., "Tip 1:", "2)", "Number 3:").
    {% endif %}
    {% if archetype == "versus" %}
    Present a balanced comparison with a clear recommendation at the end.
    The hook must be provocative; avoid neutral phrasing like "Choosing between X and Y".
    The hook should be a bold statement, not a question.
    {% endif %}
    {% if archetype == "howto" %}
    Give 4-5 clear steps that are easy to follow.
    Each step should be two sentences: the action plus a quick result/why.
    {% endif %}
    {% if archetype == "myth" %}
    Start with the myth, then reveal the reality with evidence.
    Use explicit "Myth: X" and "Reality: Y" phrasing in scene text.
    The hook should be a provocative tease, not a "Myth:" line.
    {% endif %}
    {% if archetype == "story" %}
    Use narrative structure: setup, conflict, resolution.
    The hook should be a teaser line that is not repeated in scene 1.
    {% endif %}
    {% if archetype == "hot-take" %}
    Lead with a provocative opinion, then back it up with reasoning.
    {% endif %}

    OUTPUT RULES:
    - Respond with JSON only. No markdown or code fences.
    - Spoken text fields (hook, scenes[].text, cta) must be plain text: no emojis, no markdown, no hashtags.
    - Hashtags only go in the hashtags array.
    - Each scene text must be at least 20 words.
    - Hook must be a statement (no question mark).

    Output as JSON:
    {
      "scenes": [
        {"text": "...", "visualDirection": "...", "mood": "..."}
      ],
      "reasoning": "...",
      "title": "...",
      "hook": "...",
      "cta": "...",
      "hashtags": ["#tag1", "#tag2"]
    }

# LLM providers to test against
providers:
  - id: openai:gpt-4o
    config:
      temperature: 0.7
      response_format:
        type: json_object

# Default assertions applied to all tests
defaultTest:
  options:
    # Use a cheaper model for grading (LLM-as-judge)
    provider: openai:gpt-4o-mini
  assert:
    # Layer 1: Schema validation
    - type: javascript
      value: |
        try {
          const script = JSON.parse(output);
          return (
            Array.isArray(script.scenes) &&
            script.scenes.length >= 3 &&
            script.scenes.length <= 7
          );
        } catch { return false; }
      weight: 2

    # Layer 2: Programmatic quality checks
    - type: javascript
      value: |
        try {
          const script = JSON.parse(output);
          const countWords = (text) =>
            text ? text.trim().split(/\s+/).filter(Boolean).length : 0;
          const spoken = [script.hook, ...(script.scenes || []).map((s) => s.text), script.cta]
            .filter(Boolean);
          const wordCount = spoken.reduce((acc, text) => acc + countWords(text), 0);
          return wordCount >= 90 && wordCount <= 150;
        } catch { return false; }
      weight: 1

    - type: javascript
      value: |
        try {
          const script = JSON.parse(output);
          return (script.scenes || []).every((scene) =>
            typeof scene.visualDirection === 'string' && scene.visualDirection.trim().length > 5
          );
        } catch { return false; }
      weight: 1

    - type: javascript
      value: |
        try {
          const script = JSON.parse(output);
          const spoken = [script.hook, ...(script.scenes || []).map((s) => s.text), script.cta]
            .filter(Boolean)
            .join(' ');
          const hasEmoji = /\p{Extended_Pictographic}/u.test(spoken);
          const hasMarkdown = /[`*_]/.test(spoken) || /\[(.+?)\]\((.+?)\)/.test(spoken);
          const hasHashtag = /(^|\s)#\w+/.test(spoken);
          return !hasEmoji && !hasMarkdown && !hasHashtag;
        } catch { return false; }
      weight: 2

    - type: javascript
      value: |
        try {
          const script = JSON.parse(output);
          const normalize = (text) =>
            (text || '')
              .toLowerCase()
              .replace(/[^\p{L}\p{N}\s]/gu, '')
              .replace(/\s+/g, ' ')
              .trim();
          const hook = normalize(script.hook);
          const scene1 = normalize(script.scenes?.[0]?.text);
          if (!hook || !scene1) return true;
          if (scene1.startsWith(hook) || scene1.includes(hook)) return false;
          const hookWords = hook.split(' ');
          const sceneWords = scene1.split(' ');
          if (hookWords.length >= 4 && sceneWords.length >= 4) {
            const prefixLen = Math.min(6, hookWords.length, sceneWords.length);
            const hookPrefix = hookWords.slice(0, prefixLen).join(' ');
            const scenePrefix = sceneWords.slice(0, prefixLen).join(' ');
            if (hookPrefix === scenePrefix) return false;
          }
          return true;
        } catch { return false; }
      weight: 2

    - type: javascript
      value: |
        try {
          const script = JSON.parse(output);
          return typeof script.reasoning === 'string' && script.reasoning.trim().length > 10;
        } catch { return false; }
      weight: 1

# Test cases with various topics and archetypes
tests:
  # Listicle archetype
  - vars:
      topic: '5 JavaScript tips every developer should know'
      archetype: 'listicle'
    assert:
      # Layer 3: LLM-graded quality checks
      - type: llm-rubric
        value: |
          The script opens with an attention-grabbing hook that would stop a TikTok user from scrolling.
          The first sentence should be surprising, controversial, or promise immediate value.
          Score 0 if it's a generic introduction like "In this video..." or "Today we'll cover...".
        threshold: 0.7
        weight: 2

      - type: llm-rubric
        value: |
          The language is casual, conversational, and suitable for TikTok/Reels/Shorts.
          No corporate jargon, no formal academic language, no "as a developer" phrases.
          Should sound like a friend giving advice, not a textbook.
        threshold: 0.8
        weight: 1

      - type: llm-rubric
        value: |
          Each scene has a specific, filmable visual description.
          Visual directions should describe what the viewer will SEE on screen.
          Avoid abstract concepts that cannot be filmed (like "efficiency" or "code quality").
        threshold: 0.8
        weight: 1

      - type: llm-rubric
        value: |
          For a listicle archetype, the script should have a clear numbered structure.
          Each tip should be distinct and actionable.
          The list should feel complete, not like it was cut off.
        threshold: 0.8
        weight: 1

  # Versus archetype
  - vars:
      topic: 'Redis vs PostgreSQL for caching'
      archetype: 'versus'
    assert:
      - type: llm-rubric
        value: |
          The script opens with an attention-grabbing hook.
          For a versus video, this might be a provocative statement like "You're using the wrong database"
          or a relatable problem statement.
        threshold: 0.7
        weight: 2

      - type: llm-rubric
        value: |
          For a versus archetype, the script should clearly compare both options.
          It should present pros and cons of each, not just favor one.
          The comparison should be fair and technically accurate.
        threshold: 0.8
        weight: 2

      - type: llm-rubric
        value: |
          The script provides a clear recommendation or decision framework at the end.
          Viewers should know when to use each option.
        threshold: 0.7
        weight: 1

  # Story archetype
  - vars:
      topic: 'How I learned React in 2 weeks'
      archetype: 'story'
    assert:
      - type: llm-rubric
        value: |
          The script has a narrative arc: setup, challenge, resolution.
          It should feel like a personal story, not a tutorial.
        threshold: 0.8
        weight: 2

      - type: llm-rubric
        value: |
          The language uses first-person perspective and includes emotional elements.
          The viewer should feel connected to the storyteller.
        threshold: 0.7
        weight: 1

  # Hot-take archetype
  - vars:
      topic: 'TypeScript is overrated'
      archetype: 'hot-take'
    assert:
      - type: llm-rubric
        value: |
          The script opens with a bold, controversial statement that will spark debate.
          It should immediately make viewers want to comment (agree or disagree).
        threshold: 0.8
        weight: 2

      - type: llm-rubric
        value: |
          Despite being a hot take, the script provides actual reasoning and evidence.
          It's not just rage bait; there should be substance behind the opinion.
        threshold: 0.7
        weight: 2

  # Howto archetype
  - vars:
      topic: 'How to deploy a Node.js app in 5 minutes'
      archetype: 'howto'
    assert:
      - type: llm-rubric
        value: |
          The script has clear, sequential steps that are easy to follow.
          Each step should be actionable and specific.
        threshold: 0.8
        weight: 2

      - type: llm-rubric
        value: |
          The visual directions show what the viewer should be doing at each step.
          This could include screen recordings, terminal commands, or UI interactions.
        threshold: 0.8
        weight: 1

  # Myth archetype
  - vars:
      topic: 'You need a CS degree to be a developer'
      archetype: 'myth'
    assert:
      - type: llm-rubric
        value: |
          The script clearly states the myth, then definitively debunks it.
          The structure should be "Myth: X / Reality: Y" or similar.
        threshold: 0.8
        weight: 2

      - type: llm-rubric
        value: |
          The debunking includes evidence, examples, or statistics.
          It's not just opinion; there should be factual backing.
        threshold: 0.7
        weight: 1
